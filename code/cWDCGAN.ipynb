{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whR7kGRl48kl"
      },
      "source": [
        "# Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrDxnNHG45du",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Overall code structure: based on https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "# Dataset loading: based on https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "# Conditional GAN aspect: based on https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN\n",
        "# Wasserstein aspect: based on https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan/wgan.py\n",
        "# GAN stability improvements: based on https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gulO84Mo3W4i"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pskBplxr3U4i",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torch.autograd import Variable\n",
        "from skimage import io\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puHYx_2_1-dA"
      },
      "source": [
        "# Hyperparamaters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znP4V39jCVZQ",
        "outputId": "107cb2a8-e18c-4539-de6a-310aa71e12c8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Root directory for dataset\n",
        "dataroot = \"training_data/\"\n",
        "imageroot = dataroot+\"layout_images\"\n",
        "labelroot = dataroot+\"layout_labels_all_4_text_proportion_classes.json\"\n",
        "\n",
        "# The number of classes\n",
        "num_classes = 4\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 64\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 32\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 128\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 128\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5000\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.00005\n",
        "\n",
        "# Clip value (wasserstein)\n",
        "clip_value = 0.01\n",
        "\n",
        "# Number of times to update discriminator before we update generator\n",
        "n_critic = 5\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uepKwS6R2Tvz"
      },
      "source": [
        "# Google drive integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "ulyXUpjwpaWy",
        "outputId": "0a00ec7f-3cdf-44a2-884d-ae3da5fe1a40",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a05ddf0b57e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# optional Google drive integration - this will allow you to save and resume training, and may speed up redownloading the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/Uni/L3/Project/Coded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# optional Google drive integration - this will allow you to save and resume training, and may speed up redownloading the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"drive/My Drive/Uni/L3/Project/Coded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9D9ZCVP2W44"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK383zeDM4Ac",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "class MagazineDataset(Dataset):\n",
        "    # Source: Based on https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_files = os.listdir(imageroot)\n",
        "\n",
        "        with open(labelroot) as infile:\n",
        "          self.labels = json.load(infile)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get paths for files\n",
        "        file_name = self.image_files[idx][:-4]\n",
        "        img_path = os.path.join(imageroot, file_name+\".png\")\n",
        "\n",
        "        # Read image and label\n",
        "        image = io.imread(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[file_name]\n",
        "        label = label[\"textProportion\"].index(1)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Resize(image_size),\n",
        "                torchvision.transforms.CenterCrop(image_size),\n",
        "                torchvision.transforms.Normalize(\n",
        "                    (0.5), (0.5)),\n",
        "            ])\n",
        "\n",
        "dataset = MagazineDataset(transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiUsLcr62bnf"
      },
      "source": [
        "# Example training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtJs-qxHRLXz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Plot some training images\n",
        "examples = enumerate(dataloader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(torchvision.utils.make_grid(example_data.to(\"cpu\")[:16], padding=2, normalize=True).to(\"cpu\"),(1,2,0)))\n",
        "\n",
        "for i in range(16):\n",
        "  print(example_targets[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KLwWOyl0e-X"
      },
      "source": [
        "# Network weights initialisation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ7YxN4e0jWY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "  # Source: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "      nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "      nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "      nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M72Oa3fa2ibd"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlBnxrWfD4xu",
        "outputId": "d6ade416-c0d4-4945-c363-2be9e21de098",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (deconv_z): Sequential(\n",
              "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "  )\n",
              "  (deconv_label): Sequential(\n",
              "    (0): ConvTranspose2d(4, 512, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "  )\n",
              "  (main): Sequential(\n",
              "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "    (4): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (8): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.deconv_z = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(0.2),\n",
        "            nn.Dropout(0.4),\n",
        "        )\n",
        "\n",
        "        self.deconv_label = nn.Sequential(\n",
        "            nn.ConvTranspose2d(num_classes, ngf*4, 4, 1, 0),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(0.2),\n",
        "            nn.Dropout(0.4),\n",
        "        )\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d((ngf*4)*2, ngf*4, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(0.2),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf*2),\n",
        "            nn.ReLU(0.2),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*2, nc, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        # Create label embedding\n",
        "        z = self.deconv_z(z)\n",
        "        labels = self.deconv_label(labels)\n",
        "        x = torch.cat([z, labels], dim = 1)\n",
        "\n",
        "        return self.main(x)\n",
        "\n",
        "# Create the generator\n",
        "netG = Generator().to(device)\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.02.\n",
        "netG.apply(weights_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmxHyHNh2kfv"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1r0Lct_D_YD",
        "outputId": "7439d90d-473d-4ff0-b16a-7e59cdfac5f4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (embed_x): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Dropout(p=0.4, inplace=False)\n",
              "  )\n",
              "  (embed_labels): Sequential(\n",
              "    (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Dropout(p=0.4, inplace=False)\n",
              "  )\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "    (4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (7): Dropout(p=0.4, inplace=False)\n",
              "    (8): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.embed_x = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf//2, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.embed_labels = nn.Sequential(\n",
        "            nn.Conv2d(num_classes, ndf//2, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d((ndf//2)*2, ndf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Conv2d(ndf*4, 1, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        # Create label embedding\n",
        "        x = self.embed_x(x)\n",
        "        labels = self.embed_labels(labels)\n",
        "        x = torch.cat([x, labels], dim = 1)\n",
        "\n",
        "        return self.main(x)\n",
        "\n",
        "# Create the Discriminator\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFdG4wTEI0bm"
      },
      "source": [
        "# Checkpoint functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBAvZqVbIPhp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch):\n",
        "  checkpoint = {\n",
        "      'netG': netG.state_dict(),\n",
        "      'netD': netD.state_dict(),\n",
        "      'optimizerG': optimizerG.state_dict(),\n",
        "      'optimizerD': optimizerD.state_dict()\n",
        "  }\n",
        "\n",
        "  torch.save(checkpoint, 'checkpoints/text_proportion_wasserstein_checkpoint'+str(epoch)+'.pth')\n",
        "\n",
        "def load_checkpoint(epoch):\n",
        "    checkpoint = torch.load('checkpoints/text_proportion_wasserstein_checkpoint'+str(epoch)+'.pth', map_location=torch.device(device))\n",
        "    netG.load_state_dict(checkpoint['netG'])\n",
        "    netD.load_state_dict(checkpoint['netD'])\n",
        "    optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
        "    optimizerD.load_state_dict(checkpoint['optimizerD'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD9q6E_t2pZZ"
      },
      "source": [
        "# Training initialisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOEVl1ZEEG_I",
        "outputId": "4354d9a5-6067-4229-b2cf-d690d9e7150d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerG = torch.optim.RMSprop(netG.parameters(), lr=lr)\n",
        "optimizerD = torch.optim.RMSprop(netD.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# Preprocessed labels\n",
        "onehot = torch.zeros(num_classes, num_classes)\n",
        "onehot = onehot.scatter_(1, torch.LongTensor(list(range(num_classes))).view(num_classes,1), 1).view(num_classes, num_classes, 1, 1)\n",
        "fill = torch.zeros([num_classes, num_classes, image_size, image_size])\n",
        "for i in range(num_classes):\n",
        "    fill[i, i, :, :] = 1\n",
        "\n",
        "one = torch.FloatTensor([1]).to(device)\n",
        "mone = (one * -1).to(device)\n",
        "\n",
        "def random_labels(size, min,max):\n",
        "  # Generate random labels\n",
        "  vals = torch.rand((size,), dtype=torch.float, device=device) * (max - min) + min\n",
        "  return vals\n",
        "\n",
        "# Fixed noise & label\n",
        "temp_z0_ = torch.randn(nz).repeat(4)\n",
        "temp_z1_ = torch.randn(nz).repeat(4)\n",
        "temp_z2_ = torch.randn(nz).repeat(4)\n",
        "temp_z3_ = torch.randn(nz).repeat(4)\n",
        "fixed_z_ = torch.cat([temp_z0_, temp_z1_, temp_z2_, temp_z3_], 0)\n",
        "fixed_z_ = fixed_z_.view(-1, nz, 1, 1)\n",
        "\n",
        "fixed_y_ = torch.tensor([0,1,2,3]).repeat(4)\n",
        "fixed_y_label_ = onehot[fixed_y_]\n",
        "\n",
        "fixed_noise, fixed_labels = Variable(fixed_z_.to(device), volatile=True), Variable(fixed_y_label_.to(device), volatile=True)\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMCcgV9O3CCF"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZT3QkPpeESWz",
        "outputId": "a4359d12-ce9c-4f42-e342-7af680fcfca5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Starting Training Loop...\")\n",
        "\n",
        "# Load network at checkpoint\n",
        "checkpoint = 0\n",
        "if(checkpoint):\n",
        "  load_checkpoint(checkpoint)\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(checkpoint+1, num_epochs+checkpoint):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # Get real images (x_) and real labels (y_)\n",
        "        x_ = data[0].to(device)\n",
        "        y_ = data[1].to(device)\n",
        "\n",
        "        # Get batch size\n",
        "        b_size = x_.size(0)\n",
        "\n",
        "        # Image labels (wasserstein)\n",
        "        real_img_labels = torch.full((b_size,), -1.0, dtype=torch.float, device=device)\n",
        "        false_img_labels = torch.full((b_size,), 1.0, dtype=torch.float, device=device)\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "\n",
        "        ## Train with all-real batch\n",
        "        # Forward pass real batch through D\n",
        "        netD.zero_grad()\n",
        "        y_fill_ = fill[y_]\n",
        "        x_, y_fill_ = Variable(x_.to(device)), Variable(y_fill_.to(device))\n",
        "        outputReal = netD(x_, y_fill_).view(-1)\n",
        "\n",
        "        d_loss_real = outputReal.mean(0).view(1)\n",
        "        d_loss_real.backward(one)\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        z_ = torch.randn((b_size, nz)).view(-1, nz, 1, 1)\n",
        "        y_ = (torch.rand(b_size, 1) * num_classes).type(torch.LongTensor).squeeze()\n",
        "        y_label_ = onehot[y_]\n",
        "        y_fill_ = fill[y_]\n",
        "        z_, y_label_, y_fill_ = Variable(z_.to(device)), Variable(y_label_.to(device)), Variable(y_fill_.to(device))\n",
        "\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(z_, y_label_)\n",
        "\n",
        "        # Classify all fake batch with D\n",
        "        outputFake = netD(fake.detach(), y_fill_).view(-1)\n",
        "\n",
        "        d_loss_fake = outputFake.mean(0).view(1)\n",
        "        d_loss_fake.backward(mone)\n",
        "\n",
        "        # Calculate discriminator loss\n",
        "        errD = d_loss_fake - d_loss_real\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Clip weights of discriminator\n",
        "        for p in netD.parameters():\n",
        "            p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "\n",
        "        # Update the generator every n_critic iterations\n",
        "        if i % n_critic == 0:\n",
        "            netG.zero_grad()\n",
        "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "            output = netD(fake, y_fill_).view(-1)\n",
        "\n",
        "            errG = output.mean().mean(0).view(1)\n",
        "            errG.backward(one)\n",
        "            optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if epoch % 1 == 0 and i == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'% (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item()))\n",
        "            \n",
        "            fake_visu = netG(fixed_noise, fixed_labels)\n",
        "            plt.gca().get_xaxis().set_visible(False)\n",
        "            plt.gca().get_yaxis().set_visible(False)\n",
        "            plt.imshow(torchvision.utils.make_grid(fake_visu, normalize=True, nrow=4).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
        "            plt.savefig(\"anim/\"+str(iters)+\".png\")\n",
        "            plt.show()\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "    # Save checkpoint every N epochs\n",
        "    if(epoch % 50 == 0 and epoch != 0):\n",
        "      # Save network checkpoi nt\n",
        "      save_checkpoint(epoch)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "whR7kGRl48kl",
        "gulO84Mo3W4i",
        "puHYx_2_1-dA",
        "N9D9ZCVP2W44",
        "PiUsLcr62bnf",
        "5KLwWOyl0e-X",
        "M72Oa3fa2ibd",
        "CmxHyHNh2kfv",
        "FFdG4wTEI0bm",
        "uD9q6E_t2pZZ",
        "Hcc_SnwiZvn0",
        "y0KSmPYZ25g0",
        "HFhVatsB2syA",
        "RYwUyKHc2z93"
      ],
      "name": "Text Proportion - Wasserstein cDCGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
