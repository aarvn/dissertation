import numpy as np
import matplotlib.pyplot as plt
import cv2 as cv
import torch

# Sources
# overlapping_area - https://stackoverflow.com/a/27162334
# extract_bboxes - Based on https://xtqiao.com/projects/content_aware_layout/paper.pdf

# Initialise dimension
dim = [32, 24]


def flatten_list(t):
    """Flatten a list of lists."""
    return [item for sublist in t for item in sublist]


def extract_label(label, labels):
    """Extract a mask containing a specific label only."""
    ret = labels.copy()
    for i in range(len(ret)):
        for j in range(len(ret[0])):
            if ret[i][j] != 0 and ret[i][j] != label:
                ret[i][j] = 0
            elif ret[i][j] == label:
                ret[i][j] = 1
    return ret


def avg(vals):
    """Calculate average of values of a list."""
    return sum(vals)/len(vals)


def overlapping_area(a, b):
    """Calculate the overlapping area of two axis-aligned rectangles."""
    """Source: https://stackoverflow.com/a/27162334"""
    dx = min(a[1], b[1]) - max(a[0], b[0])
    dy = min(a[3], b[3]) - max(a[2], b[2])
    if (dx >= 0) and (dy >= 0):
        return dx*dy
    else:
        return 0


def align_bboxes(bboxes, thresholds=[1, 1, 1, 1]):
    """Align the provided bounding boxes based on thresholds [top, right, bottom, left]."""
    # Extract elements
    elements = []
    for key, value in bboxes.items():
        for bbox in value:
            elements.append({
                "type": key,
                "bbox": bbox
            })

    # Align
    if(len(elements) > 1):
        # For each direction [top,left,bottom,right]
        for dir in range(4):
            clusters = [[elements[0]]]

            # Place each element in approporiate cluster (basic clustering algorithm which clusters elements that are aligned within the appropriate threshold)
            for element in elements[1:]:
                t = element["bbox"][dir]
                inserted = False

                # For each cluster
                for i in range(len(clusters)):
                    can_be_added_counter = 0
                    # For each element in the cluster
                    for j in range(len(clusters[i])):
                        e = clusters[i][j]
                        if abs(e["bbox"][dir] - t) <= thresholds[dir]:
                            can_be_added_counter += 1

                    if len(clusters[i]) == can_be_added_counter:
                        clusters[i].append(element)
                        inserted = True
                        break

                if not inserted:
                    clusters.append([element])

            # Change the new top line for each element
            for i in range(len(clusters)):
                # If more than one element in the cluster
                if(len(clusters[i]) > 1):
                    # Calculate the aligned top line
                    tops = [e["bbox"][dir] for e in clusters[i]]
                    top = int(avg(tops))

                    # Set the new top line for each item in the cluster
                    for j in range(len(clusters[i])):
                        clusters[i][j]["bbox"][dir] = top

            elements = flatten_list(clusters)

    # Reformat the bounding boxes
    bboxes = {
        "text": [],
        "image": [],
        "headline": [],
    }
    for element in elements:
        bboxes[element["type"]].append(element["bbox"])

    return bboxes


def extract_bboxes(img):
    """Run our entire refinement step on an image generated by our generative model, G."""
    def extract_bboxes_from_channel(c):
        "Extract bounding boxes from a specific channel, i.e. for a specific design element type."
        channel = image[c, :, :].view(1, image.shape[1], image.shape[2])
        channel = channel.cpu().detach().numpy().transpose(1, 2, 0)
        channel = np.uint8(channel)

        # Get connected components
        output = cv.connectedComponentsWithStats(channel, 4, cv.CV_32S)
        num_labels = output[0]
        labels = output[1]
        stats = output[2]

        # region Break weak connections
        weak_connection_threshold = 2
        labels_t = labels.transpose()

        # For each connected component, break it down into smaller components if there are weak connections
        for label_num in range(1, num_labels):
            # If the component's height or width is 1, then it is a thin component, or if it is small, we won't try to break this down
            if(stats[label_num][2] == weak_connection_threshold or stats[label_num][3] == weak_connection_threshold or stats[label_num][4] < 20):
                pass
            else:
                consec_horiz = []
                consec_vert = []

                # Calculate consecutive values horizontally
                for i in range(len(labels)):
                    max_consec = 0
                    consec_counter = 0
                    for j in range(len(labels[0])):
                        if labels[i][j] == label_num:
                            consec_counter += 1
                            max_consec = max(max_consec, consec_counter)
                        else:
                            consec_counter = 0

                    consec_horiz.append(max_consec)

                # Calculate consecutive values vertically
                for i in range(len(labels_t)):
                    max_consec = 0
                    consec_counter = 0
                    for j in range(len(labels_t[0])):
                        if labels_t[i][j] == label_num:
                            consec_counter += 1
                            max_consec = max(max_consec, consec_counter)
                        else:
                            consec_counter = 0

                    consec_vert.append(max_consec)

                # Replace weak connections
                for i in range(len(labels)):
                    for j in range(len(labels[0])):
                        # If fewer than N consecutive 1s in either direction
                        if (consec_vert[j] <= weak_connection_threshold or consec_horiz[i] <= weak_connection_threshold) and labels[i][j] == label_num:
                            labels[i][j] = 0

        # Calculate new connected components without weak connections
        labels = np.uint8(labels)
        output = cv.connectedComponentsWithStats(labels, 4, cv.CV_32S)
        num_labels = output[0]
        labels = output[1]
        stats = output[2]
        # endregion

        # Initialise elements array
        elements = []

        # Extract elements
        for i in range(1, num_labels):
            stat = stats[i]
            exact_area = stat[4]

            # Ignore elements with a small area or which are very thin
            if exact_area < 5 or stat[2] < 2:
                pass
            else:
                boundary_l = stat[0]
                boundary_r = stat[0] + stat[2]
                boundary_t = stat[1]
                boundary_b = stat[1] + stat[3]

                elements.append(
                    [boundary_l, boundary_r, boundary_t, boundary_b])

        # Align items of the same type to one another
        # Create dictionary with text key in order to be able to use align_bboxes with just one set of elements
        bboxes = {"text": elements}
        bboxes = align_bboxes(bboxes, [1, 1, 2, 2])
        elements = bboxes["text"]

        return elements

    # Create copy to avoid distorting the original
    image = torch.clone(img)

    # Round all values to be integers
    image = torch.clamp(torch.round(image), 0, 1)

    # Extract bboxes for each design element type from each channel
    bboxes = {
        "text": extract_bboxes_from_channel(0),
        "headline": extract_bboxes_from_channel(1),
        "image": extract_bboxes_from_channel(2),
    }

    # region Remove images with small visible areas (i.e. heavily occluded images)
    images = []
    for image in bboxes["image"]:
        area = (image[1]-image[0])*(image[3]-image[2])

        # Remove area of image which is covered by text or headlines
        for text in bboxes["text"]:
            area -= overlapping_area(image, text)
        for headline in bboxes["headline"]:
            area -= overlapping_area(image, headline)

        if area >= 5:
            images.append(image)

    bboxes["image"] = images
    # endregion

    # Align all items to one another (run it twice for improved alignment)
    bboxes = align_bboxes(bboxes)
    bboxes = align_bboxes(bboxes)

    return bboxes


def extract_bboxes_b1(image):
    """Extract bboxes from our generator's output without our refinement step - use basic bounding boxes only."""
    # Extract bboxes for baseline 1
    def extract_bboxes_from_channel(c):
        channel = image[c, :, :].view(1, image.shape[1], image.shape[2])
        channel = channel.cpu().detach().numpy().transpose(1, 2, 0)
        channel = np.uint8(channel)

        # Get connected components
        output = cv.connectedComponentsWithStats(channel, 4, cv.CV_32S)
        num_labels = output[0]
        stats = output[2]

        # Initialise elements array
        elements = []

        # Extract elements via bounding box
        for i in range(1, num_labels):
            stat = stats[i]

            boundary_l = stat[0]
            boundary_r = stat[0] + stat[2]
            boundary_t = stat[1]
            boundary_b = stat[1] + stat[3]

            elements.append(
                [boundary_l, boundary_r, boundary_t, boundary_b])

        return elements

    # Round all values to be integers
    image = torch.clamp(torch.round(image), 0, 1)

    bboxes = {
        "text": extract_bboxes_from_channel(0),
        "headline": extract_bboxes_from_channel(1),
        "image": extract_bboxes_from_channel(2),
    }

    return bboxes


def boundary_to_bbox(boundary):
    """Convert [top,left,right,bottom] boundary to [[x,y],[x+w,y],[x,y+h],[x+w,y+h]] bounding box."""
    return [
        [boundary[0], boundary[2]],
        [boundary[1]-1, boundary[2]],
        [boundary[1]-1, boundary[3]-1],
        [boundary[0], boundary[3]-1],
    ]


def show_bboxes(bboxes):
    """Create a rendering of bounding boxes."""
    boundary_img = np.zeros((dim[0], dim[1], 3), dtype='uint8')

    for element in bboxes["image"]:
        e = np.array(boundary_to_bbox(element), dtype='int32')
        cv.fillPoly(boundary_img, [e], color=(0, 0, 255))  # blue

    for element in bboxes["text"]:
        e = np.array(boundary_to_bbox(element), dtype='int32')
        cv.fillPoly(boundary_img, [e], color=(255, 0, 0))  # red

    for element in bboxes["headline"]:
        e = np.array(boundary_to_bbox(element), dtype='int32')
        cv.fillPoly(boundary_img, [e], color=(0, 255, 0))  # green

    plt.imshow(boundary_img, cmap=plt.cm.binary)
    plt.show()
